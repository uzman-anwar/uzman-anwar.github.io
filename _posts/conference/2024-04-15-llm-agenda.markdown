---
layout: post
title:  "Foundational Challenges in Assuring Alignment and Safety of Large Language Models"
date:   2024-15-04 14:30:00
image: /papers/LLM-Agenda/shoggath.png
categories: conference
author: "Usman Anwar"
authors: "<strong>Usman Anwar</strong> and 41 other authors"
venue: "Under Submission"
arxiv: https://arxiv.org/abs/2404.09932
website: https://llm-safety-challenges.github.io/
tweetprint: https://x.com/usmananwar391/status/1780254437895713011
---
This 150+ pages long work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose 200+, concrete research questions.
